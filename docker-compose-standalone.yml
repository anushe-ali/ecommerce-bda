version: "3.8"

services:
  airflow:
    build:
      context: .
      dockerfile: airflow/Dockerfile.airflow
    container_name: airflow
    restart: always
    ports:
      - "8085:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./scripts:/app
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
    command: >
      bash -c "
      airflow db init &&
      airflow users create
        --username admin
        --password admin
        --firstname admin
        --lastname admin
        --role Admin
        --email admin@airflow.com &&
      airflow scheduler & airflow webserver
      "
    depends_on:
      - mongo
      - spark-master-bda
      - kafka

  archiver:
    build:
      context: ./scripts
      dockerfile: Dockerfile.archiver
    container_name: ecommerce-archiver
    depends_on:
      - mongo
      - namenode
    networks:
      - bda_network
    restart: always

  zookeeperbda:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeperbda
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - bda_network
    ports:
      - "32181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeperbda
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeperbda:2181

      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092

      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    networks:
      - bda_network

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8
    container_name: namenode
    hostname: namenode
    environment:
      - NAMENODE_HOSTNAME=namenode
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    env_file:
      - ./hadoop.env
    ports:
      - "9000:9000"
      - "50070:50070"
    networks:
      - bda_network

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop2.7.4-java8
    container_name: datanode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    env_file:
      - ./hadoop.env
    networks:
      - bda_network

  resourcemanager:
    image: bde2020/hadoop-resourcemanager:2.0.0-hadoop2.7.4-java8
    container_name: resourcemanager
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
    env_file:
      - ./hadoop.env
    networks:
      - bda_network

  nodemanager1:
    image: bde2020/hadoop-nodemanager:2.0.0-hadoop2.7.4-java8
    container_name: nodemanager
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - bda_network

  historyserver:
    image: bde2020/hadoop-historyserver:2.0.0-hadoop2.7.4-java8
    container_name: historyserver
    volumes:
      - hadoop_historyserver:/hadoop/yarn/timeline
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075 resourcemanager:8088"
    env_file:
      - ./hadoop.env
    networks:
      - bda_network

  hbase:
    image: bde2020/hbase-standalone:1.0.0-hbase1.2.6
    container_name: hbase
    volumes:
      - hbase_data:/hbase-data
      - hbase_zookeeper_data:/zookeeper-data
    ports:
      - 16000:16000
      - 16010:16010
      - 16020:16020
      - 16030:16030
      - 2888:2888
      - 3888:3888
      - 2182:2181
      - 9090:9090
    environment:
      SERVICE_PRECONDITION: "namenode:50070 datanode:50075"
    env_file:
      - ./hbase-standalone.env
    networks:
      - bda_network

  spark-master-bda:
    platform: linux/amd64
    image: bde2020/spark-master:3.3.0-hadoop3.3
    container_name: spark-master-bda
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master-bda
    networks:
      - bda_network
    volumes:
      - ./libs:/opt/spark/jars
      - ./scripts:/app/scripts

  spark-worker-bda1:
    platform: linux/amd64
    image: bde2020/spark-worker:3.3.0-hadoop3.3
    container_name: spark-worker-bda1
    depends_on:
      - spark-master-bda
    ports:
      - "8081:8081"
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master-bda:7077
      - SPARK_MASTER=spark://spark-master-bda:7077
    networks:
      - bda_network
    volumes:
      - ./libs:/opt/spark/jars
      - ./scripts:/app/scripts

  data-generator:
    build:
      context: ./scripts
      dockerfile: Dockerfile.generator
    container_name: ecommerce-data-generator
    depends_on:
      - kafka
    restart: always
    networks:
      - bda_network

  kafka-consumer:
    build:
      context: ./scripts
      dockerfile: Dockerfile.generator
    container_name: kafka-consumer
    depends_on:
      - kafka
      - zookeeperbda
    command: python kafkaconsum.py
    restart: always
    networks:
      - bda_network

  mongo:
    image: mongo:4.4
    container_name: mongo
    ports:
      - "27017:27017"
    networks:
      - bda_network

  dashboard:
    build:
      context: ./scripts
      dockerfile: Dockerfile.dashboard
    container_name: ecommerce-dashboard
    depends_on:
      - mongo
    ports:
      - "8050:8050"
    networks:
      - bda_network
    restart: always

volumes:
  hadoop_namenode:
  hadoop_datanode:
  hadoop_historyserver:
  hbase_data:
  hbase_zookeeper_data:

networks:
  bda_network:
    driver: bridge
    name: bda_network

